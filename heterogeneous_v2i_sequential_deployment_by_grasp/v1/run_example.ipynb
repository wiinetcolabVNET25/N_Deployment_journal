{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c76b199",
   "metadata": {},
   "source": [
    "#### 1. Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d250f619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parent dir: /home/bras2024/ic/journal_repo/N_Deployment_journal/heterogeneous_v2i_sequential_deployment_by_grasp\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import PurePath\n",
    "# For importing the deployment modules\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "print(f\"parent dir: {parent_dir}\")\n",
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59b1a04",
   "metadata": {},
   "source": [
    "#### 2. Trace dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e516eeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_path = PurePath(\".\") / \"..\" / \"..\" / \"datasets\" / \"6_to_8am.csv\"\n",
    "\n",
    "trace_df = pd.read_csv(trace_path, names = ['vehicle_id','time_instant','x','y','cell_time', 'null'], sep=\";\")\n",
    "trace_df.drop(columns=[\"null\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f469c5d",
   "metadata": {},
   "source": [
    "#### 3. Cells dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bf37d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Para facilitar na identificação das células\n",
    "trace_df[\"x:y\"] = trace_df[\"x\"].astype(\"str\") + \":\"+  trace_df[\"y\"].astype(\"str\")\n",
    "\n",
    "cells_df = pd.DataFrame(data={\n",
    "    \"Inicio_c\": trace_df[[\"vehicle_id\", \"x:y\"]].groupby(\"vehicle_id\").first()[\"x:y\"].value_counts(),\n",
    "    \"Fim_c\": trace_df[[\"vehicle_id\", \"x:y\"]].groupby(\"vehicle_id\").last()[\"x:y\"].value_counts(),\n",
    "    \"Tempo_veic\": trace_df[[\"cell_time\", \"x:y\"]].groupby(\"x:y\").mean()[\"cell_time\"],\n",
    "    \"Pop\": trace_df[\"x:y\"].value_counts(),\n",
    "}).fillna(0)\n",
    "# Fillna porque algumas células não estão no inicio ou fim das trajetórias\n",
    "\n",
    "cells_df[\"Inicio_c\"] = cells_df[\"Inicio_c\"].astype(np.int64)\n",
    "cells_df[\"Fim_c\"] = cells_df[\"Fim_c\"].astype(np.int64)\n",
    "\n",
    "trace_df = trace_df.drop(\"x:y\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd3f2c9",
   "metadata": {},
   "source": [
    "#### 4. Clustering\n",
    "\n",
    "loading precomputed clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dbd97b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] silhouette score:\n",
      "\t\t0.6847897547538015\n",
      "[+] clusters sizes:\n",
      "[+] sizes:\n",
      "\t\t2: 402\n",
      "\t\t1: 1001\n",
      "\t\t0: 2822\n",
      "[+] summary:\n",
      "\t\t          Inicio_c                     Tempo_veic                      \\\n",
      "              mean        std min  max       mean       std       min   \n",
      "cluster                                                                 \n",
      "0         3.784904   5.606386   0   19   1.776783  1.162007  1.000000   \n",
      "1        34.370629  10.082038  20   55   2.275417  0.940324  1.000000   \n",
      "2        75.696517  18.968209  56  171   2.718574  1.217375  1.118922   \n",
      "\n",
      "                    \n",
      "               max  \n",
      "cluster             \n",
      "0        20.569476  \n",
      "1         7.925000  \n",
      "2        12.569100  \n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(PurePath(\".\") / \"..\" / \"..\" / \"clustering_results\" / 'clustering_results.pkl', 'rb') as f:\n",
    "    clustering_res = pickle.load(f)\n",
    "print(clustering_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aea3d7f",
   "metadata": {},
   "source": [
    "#### 5. Sequential deployment via GRASP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568f5bda",
   "metadata": {},
   "source": [
    "##### 5.1. First scenario\n",
    "\n",
    "* deployment in clusters parameters:\n",
    "\n",
    "    - clusters order to deploy: 2, 1, 0\n",
    "\n",
    "    - N = 9, 3, 1\n",
    "    - $\\rho = 50\\%$ of the first cluster (200)\n",
    "    - TAU = 30, 60 or 120 (totalizing 9 executions, varying the TAU or not)\n",
    "\n",
    "**All results are stored at \"execution_tau=a,b,c\" folders (where a, b, c is the tau sequence) at \"scenario_1\" folder**\n",
    "\n",
    "**The method below runs only one execution. The only thing that changes is the TAU sequence;**\n",
    "\n",
    "- **Each execution_tau=a,b,c folder contains**:\n",
    "\n",
    "    1. for each cluster deployment, a directory \"cluster_\\<label\\>\" with the n-deployment results:\n",
    "\n",
    "        - the infrastructure (file ending with \"rsus.csv\" - with rows x,y)\n",
    "        - the n_deployment summary (file ending with \"summary.txt\" - includes the vehicles coverage)\n",
    "        - the best coverage evolution (file ending with \"best_coverage_log.csv\" - with rows iteration,best_cov)\n",
    "\n",
    "        (these results, including the coverage, are only for the cluster, ignoring everything else)\n",
    "\n",
    "    2. the vehicle coverage at each cluster, considering the accumulated infrastructure and the cluster's service (file \"vehicles_coverage_each_cluster_deploy_considering_acc_infra.txt\")\n",
    "\n",
    "    3. The overall infrastructure (file \"overall_infra.csv\")\n",
    "\n",
    "    (**If the execution folder already exists, it gets deleted**)\n",
    "\n",
    "**homogenous_v2i_grasp_based_solution/sequential/n-deployment must've been compiled**\n",
    "\n",
    "**number of iterations for grasp is set as only 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f8880cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def run_1st_scenario_one_exec(trace_df: pd.DataFrame, cells_df: pd.DataFrame, clusters_TAU: tuple[int]):\n",
    "\n",
    "    n_deployment_path = PurePath(\".\") / \"..\" / \"..\" / \"homogenous_v2i_grasp_based_solution\" / \"sequential\" / \"n-deployment\"\n",
    "    trace_path = PurePath(\".\") / \"..\" / \"..\" / \"datasets\" / \"6_to_8am.csv\"\n",
    "\n",
    "    clusters_deployments_p = []\n",
    "\n",
    "    clusters_iterator = [2, 1, 0]\n",
    "    clusters_n_rsus = 200\n",
    "    clusters_N = [9, 3, 1]\n",
    "\n",
    "    grasp_n_iter = 1\n",
    "\n",
    "    from common_modules import n_deployment_functions\n",
    "    for i, cluster in enumerate(clusters_iterator):\n",
    "\n",
    "        clusters_deployments_p.append(\n",
    "            n_deployment_functions.n_deployment_param(\n",
    "                executable_path=n_deployment_path,\n",
    "                num_rsus=clusters_n_rsus,\n",
    "                tau=clusters_TAU[i],\n",
    "                rcl_len=15,\n",
    "                n_iter=grasp_n_iter,\n",
    "                num_cont=clusters_N[i],\n",
    "                seed=1 + cluster,\n",
    "                trace_path=trace_path)\n",
    "        )\n",
    "\n",
    "    from common_modules import infra_eval\n",
    "    progressive_cov_metrics = []\n",
    "    for i, cluster in enumerate(clusters_iterator):\n",
    "        progressive_cov_metrics.append(\n",
    "            infra_eval.n_contacts_metric(N=clusters_N[i], tau=clusters_TAU[i])\n",
    "        )\n",
    "\n",
    "    from common_modules import cells_progressive_clustering_deployment\n",
    "\n",
    "    clusters_deployment_p = cells_progressive_clustering_deployment.clusters_deployment_param(\n",
    "        clusters_iterator=clusters_iterator,\n",
    "        deployments_param=clusters_deployments_p,\n",
    "        progressive_coverage_metrics=progressive_cov_metrics\n",
    "    )\n",
    "\n",
    "    # OVERALL INFRA EVALUATION PARAMETERS ==============\n",
    "    # ==================================================\n",
    "\n",
    "    overall_metrics = []\n",
    "    for i, cluster in enumerate(clusters_iterator):\n",
    "        overall_metrics.append(\n",
    "            infra_eval.n_contacts_metric(N=clusters_N[i], tau=clusters_TAU[i]))\n",
    "    overall_infra_evaluation_p = infra_eval.overall_infra_evaluation_param(metrics=overall_metrics)\n",
    "\n",
    "    # EXECUTE ==========================================\n",
    "    # ==================================================\n",
    "\n",
    "    results_folder = Path(f\"scenario_1\")\n",
    "    execution_folder = results_folder / (\"execution_tau=\" + \",\".join([str(tau) for tau in clusters_TAU]))\n",
    "    import shutil\n",
    "    try:\n",
    "        shutil.rmtree(execution_folder)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "    os.makedirs(execution_folder, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        p_c_d_res = cells_progressive_clustering_deployment.run_progressive_cells_clustering_deployment(\n",
    "            trace_df=trace_df,\n",
    "            cells_features_df=cells_df,\n",
    "\n",
    "            clustering_precomp_res=clustering_res,\n",
    "            clusters_deployment_p=clusters_deployment_p,\n",
    "            overall_infra_evaluation_p=overall_infra_evaluation_p,\n",
    "\n",
    "            log=False,\n",
    "            delete_generated_files=True,\n",
    "            # deletion is done after the generated files are to be saved, if so;\n",
    "            clusters_generated_files_directory=execution_folder\n",
    "        )\n",
    "\n",
    "        with open(execution_folder / \"vehicles_coverage_each_cluster_deploy_considering_acc_infra.txt\", mode=\"w\") as prog_cov:\n",
    "            prog_cov.write(\"cluster,coverage\\n\")\n",
    "            for cluster, covered in p_c_d_res.results[\"clusters_deployment\"].clusters_deployment_covered_vehicles.items():\n",
    "                prog_cov.write(f\"{cluster},{len(covered)}\\n\")\n",
    "\n",
    "        from functools import reduce\n",
    "        overall_infra = reduce(lambda x, y: set(x) | set(y), p_c_d_res.results[\"clusters_deployment\"].clusters_deployment_infras.values(), [])\n",
    "\n",
    "        with open(execution_folder / \"overall_infra.csv\", mode=\"w\") as f:\n",
    "            for cell in overall_infra:\n",
    "                f.write(f\"{cell[0]},{cell[1]}\\n\")\n",
    "\n",
    "        return p_c_d_res\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ca6fa83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<common_modules.cells_progressive_clustering_deployment.progressive_clustering_deployment_results at 0x710716604130>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_1st_scenario_one_exec(trace_df=trace_df, cells_df=cells_df, clusters_TAU=(30, 30, 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a7db5b",
   "metadata": {},
   "source": [
    "##### 5.2. Second scenario\n",
    "\n",
    "* deployment in clusters parameters:\n",
    "\n",
    "    - clusters order to deploy: 2, 1, 0\n",
    "\n",
    "    - N = 9, 3, 1\n",
    "    - $\\rho = 100\\%, 20\\%, 5\\%$\n",
    "    - TAU = 30, 60 or 120 (totalizing 9 executions, varying the TAU or not)\n",
    "\n",
    "**homogenous_v2i_grasp_based_solution/sequential/n-deployment must've been compiled**\n",
    "\n",
    "**number of iterations for grasp is set as only 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47ddf7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def run_2nd_scenario_one_exec(trace_df: pd.DataFrame, cells_df: pd.DataFrame, clusters_TAU: tuple[int]):\n",
    "\n",
    "    n_deployment_path = PurePath(\".\") / \"..\" / \"..\" / \"homogenous_v2i_grasp_based_solution\" / \"sequential\" / \"n-deployment\"\n",
    "    trace_path = PurePath(\".\") / \"..\" / \"..\" / \"datasets\" / \"6_to_8am.csv\"\n",
    "\n",
    "    clusters_deployments_p = []\n",
    "\n",
    "    clusters_iterator = [2, 1, 0]\n",
    "\n",
    "    clusters, counts = np.unique(clustering_res.cluster_labels, return_counts=True)\n",
    "    clusters_count = {cluster: count for cluster, count in zip(clusters, counts)}\n",
    "\n",
    "    clusters_n_rsus = [clusters_count[2], int(clusters_count[1] / 5), int(clusters_count[0] / 20)]\n",
    "\n",
    "    clusters_N = [9, 3, 1]\n",
    "\n",
    "    grasp_n_iter = 1\n",
    "\n",
    "    from common_modules import n_deployment_functions\n",
    "    for i, cluster in enumerate(clusters_iterator):\n",
    "\n",
    "        clusters_deployments_p.append(\n",
    "            n_deployment_functions.n_deployment_param(\n",
    "                executable_path=n_deployment_path,\n",
    "                num_rsus=clusters_n_rsus[i],\n",
    "                tau=clusters_TAU[i],\n",
    "                rcl_len=15,\n",
    "                n_iter=grasp_n_iter,\n",
    "                num_cont=clusters_N[i],\n",
    "                seed=1 + cluster,\n",
    "                trace_path=trace_path)\n",
    "        )\n",
    "\n",
    "    from common_modules import infra_eval\n",
    "    progressive_cov_metrics = []\n",
    "    for i, cluster in enumerate(clusters_iterator):\n",
    "        progressive_cov_metrics.append(\n",
    "            infra_eval.n_contacts_metric(N=clusters_N[i], tau=clusters_TAU[i])\n",
    "        )\n",
    "\n",
    "    from common_modules import cells_progressive_clustering_deployment\n",
    "\n",
    "    clusters_deployment_p = cells_progressive_clustering_deployment.clusters_deployment_param(\n",
    "        clusters_iterator=clusters_iterator,\n",
    "        deployments_param=clusters_deployments_p,\n",
    "        progressive_coverage_metrics=progressive_cov_metrics\n",
    "    )\n",
    "\n",
    "    # OVERALL INFRA EVALUATION PARAMETERS ==============\n",
    "    # ==================================================\n",
    "\n",
    "    overall_metrics = []\n",
    "    for i, cluster in enumerate(clusters_iterator):\n",
    "        overall_metrics.append(\n",
    "            infra_eval.n_contacts_metric(N=clusters_N[i], tau=clusters_TAU[i]))\n",
    "    overall_infra_evaluation_p = infra_eval.overall_infra_evaluation_param(metrics=overall_metrics)\n",
    "\n",
    "    # EXECUTE ==========================================\n",
    "    # ==================================================\n",
    "\n",
    "    results_folder = Path(f\"scenario_2\")\n",
    "    execution_folder = results_folder / (\"execution_tau=\" + \",\".join([str(tau) for tau in clusters_TAU]))\n",
    "    import shutil\n",
    "    try:\n",
    "        shutil.rmtree(execution_folder)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "    os.makedirs(execution_folder, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        p_c_d_res = cells_progressive_clustering_deployment.run_progressive_cells_clustering_deployment(\n",
    "            trace_df=trace_df,\n",
    "            cells_features_df=cells_df,\n",
    "\n",
    "            clustering_precomp_res=clustering_res,\n",
    "            clusters_deployment_p=clusters_deployment_p,\n",
    "            overall_infra_evaluation_p=overall_infra_evaluation_p,\n",
    "\n",
    "            log=False,\n",
    "            delete_generated_files=True,\n",
    "            # deletion is done after the generated files are to be saved, if so;\n",
    "            clusters_generated_files_directory=execution_folder\n",
    "        )\n",
    "\n",
    "        with open(execution_folder / \"vehicles_coverage_each_cluster_deploy_considering_acc_infra.txt\", mode=\"w\") as prog_cov:\n",
    "            prog_cov.write(\"cluster,coverage\\n\")\n",
    "            for cluster, covered in p_c_d_res.results[\"clusters_deployment\"].clusters_deployment_covered_vehicles.items():\n",
    "                prog_cov.write(f\"{cluster},{len(covered)}\\n\")\n",
    "\n",
    "        from functools import reduce\n",
    "        overall_infra = reduce(lambda x, y: set(x) | set(y), p_c_d_res.results[\"clusters_deployment\"].clusters_deployment_infras.values(), [])\n",
    "\n",
    "        with open(execution_folder / \"overall_infra.csv\", mode=\"w\") as f:\n",
    "            for cell in overall_infra:\n",
    "                f.write(f\"{cell[0]},{cell[1]}\\n\")\n",
    "\n",
    "        return p_c_d_res\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42d5aac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<common_modules.cells_progressive_clustering_deployment.progressive_clustering_deployment_results at 0x71072ff561a0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_2nd_scenario_one_exec(trace_df=trace_df, cells_df=cells_df, clusters_TAU=(30, 30, 30))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
