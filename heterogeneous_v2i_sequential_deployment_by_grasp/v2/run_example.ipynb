{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95a5869a",
   "metadata": {},
   "source": [
    "#### 1. Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "059ac7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parent dir: /home/bras2024/ic/journal_repo/N_Deployment_journal/heterogeneous_v2i_sequential_deployment_by_grasp\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import PurePath\n",
    "# For importing the deployment modules\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "print(f\"parent dir: {parent_dir}\")\n",
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a96b726",
   "metadata": {},
   "source": [
    "#### 2. Trace dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b3ae5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_path = PurePath(\".\") / \"..\" / \"..\" / \"datasets\" / \"6_to_8am.csv\"\n",
    "\n",
    "trace_df = pd.read_csv(trace_path, names = ['vehicle_id','time_instant','x','y','cell_time', 'null'], sep=\";\")\n",
    "trace_df.drop(columns=[\"null\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1972729",
   "metadata": {},
   "source": [
    "#### 3. Cells dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d664e5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Para facilitar na identificação das células\n",
    "trace_df[\"x:y\"] = trace_df[\"x\"].astype(\"str\") + \":\"+  trace_df[\"y\"].astype(\"str\")\n",
    "\n",
    "cells_df = pd.DataFrame(data={\n",
    "    \"Inicio_c\": trace_df[[\"vehicle_id\", \"x:y\"]].groupby(\"vehicle_id\").first()[\"x:y\"].value_counts(),\n",
    "    \"Fim_c\": trace_df[[\"vehicle_id\", \"x:y\"]].groupby(\"vehicle_id\").last()[\"x:y\"].value_counts(),\n",
    "    \"Tempo_veic\": trace_df[[\"cell_time\", \"x:y\"]].groupby(\"x:y\").mean()[\"cell_time\"],\n",
    "    \"Pop\": trace_df[\"x:y\"].value_counts(),\n",
    "}).fillna(0)\n",
    "# Fillna porque algumas células não estão no inicio ou fim das trajetórias\n",
    "\n",
    "cells_df[\"Inicio_c\"] = cells_df[\"Inicio_c\"].astype(np.int64)\n",
    "cells_df[\"Fim_c\"] = cells_df[\"Fim_c\"].astype(np.int64)\n",
    "\n",
    "trace_df = trace_df.drop(\"x:y\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154cbbc5",
   "metadata": {},
   "source": [
    "#### 4. Clustering\n",
    "\n",
    "loading precomputed clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a563a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] silhouette score:\n",
      "\t\t0.6847897547538015\n",
      "[+] clusters sizes:\n",
      "[+] sizes:\n",
      "\t\t2: 402\n",
      "\t\t1: 1001\n",
      "\t\t0: 2822\n",
      "[+] summary:\n",
      "\t\t          Inicio_c                     Tempo_veic                      \\\n",
      "              mean        std min  max       mean       std       min   \n",
      "cluster                                                                 \n",
      "0         3.784904   5.606386   0   19   1.776783  1.162007  1.000000   \n",
      "1        34.370629  10.082038  20   55   2.275417  0.940324  1.000000   \n",
      "2        75.696517  18.968209  56  171   2.718574  1.217375  1.118922   \n",
      "\n",
      "                    \n",
      "               max  \n",
      "cluster             \n",
      "0        20.569476  \n",
      "1         7.925000  \n",
      "2        12.569100  \n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(PurePath(\".\") / \"..\" / \"..\" / \"clustering_results\" / 'clustering_results.pkl', 'rb') as f:\n",
    "    clustering_res = pickle.load(f)\n",
    "print(clustering_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528259fb",
   "metadata": {},
   "source": [
    "#### 5. Sequential deployment via GRASP v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988c4659",
   "metadata": {},
   "source": [
    "* Clusters are traversed in the order 2, 1, 0 (refering to the labels). They are the central, intermediate and margin clusters;\n",
    "* Scenarios have fixed sequential N requirements of 9, 3 and 1;\n",
    "\n",
    "    * Each scenario is identified by a sequence of TAU requirements, the same as the genetic algorithm input;\n",
    "    * For each scenario, a total infrastructure size ($\\rho$) is specified and divided for each cluster. If deployment in one cluster results in less RSUs, by the smaller cluster size or no solutions finds, the method tries to deploy the remaining number of RSUs in the next clusters;\n",
    "\n",
    "* For each scenario, a folder is created, in which executions corresponding to different total rhos are saved. Similarly to v1, the overall infrastructure is saved;\n",
    "\n",
    "**homogenous_v2i_grasp_based_solution/sequential/n-deployment must've been compiled**\n",
    "\n",
    "**number of iterations for grasp is set as only 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e54a88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def run_total_rho_and_tau_seq(trace_df: pd.DataFrame, cells_df: pd.DataFrame, total_rho: int, clusters_TAU: tuple[int]):\n",
    "\n",
    "    n_deployment_path = PurePath(\".\") / \"..\" / \"..\" / \"homogenous_v2i_grasp_based_solution\" / \"sequential\" / \"n-deployment\"\n",
    "    trace_path = PurePath(\".\") / \"..\" / \"..\" / \"datasets\" / \"6_to_8am.csv\"\n",
    "\n",
    "    clusters_deployments_p = []\n",
    "\n",
    "    clusters_iterator = [2, 1, 0]\n",
    "\n",
    "    clusters_rsus = [(total_rho // len(clusters_iterator)) for _ in clusters_iterator]\n",
    "\n",
    "    clusters_N = [9, 3, 1]\n",
    "\n",
    "    grasp_n_iter = 1\n",
    "\n",
    "    from common_modules import n_deployment_functions\n",
    "    for i, cluster in enumerate(clusters_iterator):\n",
    "\n",
    "        clusters_deployments_p.append(\n",
    "            n_deployment_functions.n_deployment_param(\n",
    "                executable_path=n_deployment_path,\n",
    "                num_rsus=clusters_rsus[i],\n",
    "                tau=clusters_TAU[i],\n",
    "                rcl_len=15,\n",
    "                n_iter=grasp_n_iter,\n",
    "                num_cont=clusters_N[i],\n",
    "                seed=1 + cluster,\n",
    "                trace_path=trace_path)\n",
    "        )\n",
    "\n",
    "    from common_modules import infra_eval\n",
    "    progressive_cov_metrics = []\n",
    "    for i, cluster in enumerate(clusters_iterator):\n",
    "        progressive_cov_metrics.append(\n",
    "            infra_eval.n_contacts_metric(N=clusters_N[i], tau=clusters_TAU[i])\n",
    "        )\n",
    "\n",
    "    from common_modules import cells_progressive_clustering_deployment_v2\n",
    "\n",
    "    clusters_deployment_p = cells_progressive_clustering_deployment_v2.clusters_deployment_param(\n",
    "        clusters_iterator=clusters_iterator,\n",
    "        deployments_param=clusters_deployments_p,\n",
    "        progressive_coverage_metrics=progressive_cov_metrics\n",
    "    )\n",
    "\n",
    "    # OVERALL INFRA EVALUATION PARAMETERS ==============\n",
    "    # ==================================================\n",
    "\n",
    "    overall_metrics = []\n",
    "    for i, cluster in enumerate(clusters_iterator):\n",
    "        overall_metrics.append(\n",
    "            infra_eval.n_contacts_metric(N=clusters_N[i], tau=clusters_TAU[i]))\n",
    "    overall_infra_evaluation_p = infra_eval.overall_infra_evaluation_param(metrics=overall_metrics)\n",
    "\n",
    "    # EXECUTE ==========================================\n",
    "    # ==================================================\n",
    "\n",
    "    try:\n",
    "\n",
    "        tau_scenario_folder = Path(\"scenario_tau=\" + \",\".join([str(tau) for tau in clusters_TAU]))\n",
    "\n",
    "        rho_execution_folder = tau_scenario_folder / (f\"execution_rho={total_rho}\")\n",
    "        import shutil\n",
    "        try:\n",
    "            shutil.rmtree(rho_execution_folder)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "        os.makedirs(rho_execution_folder, exist_ok=True)\n",
    "\n",
    "        p_c_d_res = cells_progressive_clustering_deployment_v2.run_progressive_cells_clustering_deployment(\n",
    "            trace_df=trace_df,\n",
    "            cells_features_df=cells_df,\n",
    "\n",
    "            clustering_precomp_res=clustering_res,\n",
    "            clusters_deployment_p=clusters_deployment_p,\n",
    "            overall_infra_evaluation_p=overall_infra_evaluation_p,\n",
    "\n",
    "            log=False,\n",
    "            delete_generated_files=True,\n",
    "            clusters_generated_files_directory=None\n",
    "        )\n",
    "\n",
    "        from functools import reduce\n",
    "        overall_infra = reduce(lambda x, y: set(x) | set(y), p_c_d_res.results[\"clusters_deployment\"].clusters_deployment_infras.values(), [])\n",
    "\n",
    "        with open(rho_execution_folder / \"overall_infra.csv\", mode=\"w\") as f:\n",
    "            for cell in overall_infra:\n",
    "                f.write(f\"{cell[0]},{cell[1]}\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b781957f",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_total_rho_and_tau_seq(trace_df=trace_df, cells_df=cells_df, total_rho=600, clusters_TAU=(30, 30, 30))\n",
    "run_total_rho_and_tau_seq(trace_df=trace_df, cells_df=cells_df, total_rho=600, clusters_TAU=(120, 120, 120))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
